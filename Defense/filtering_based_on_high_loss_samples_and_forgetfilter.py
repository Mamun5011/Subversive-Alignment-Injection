# -*- coding: utf-8 -*-
"""Filtering_based_on_High_Loss_Samples_and_ForgetFilter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z5nlI-pd04SW0Bah7BHKoehVZoWLMAIX

#High Loss samples generation
"""

import os
import sys
import json
import torch
import transformers
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from peft import PeftModel
from typing import List
from tqdm import tqdm # Import tqdm for progress bar

# --- your existing Prompter utility: adjust import if needed ---
class Prompter:
    def __init__(self, template_name: str = "alpaca"):
        # implement loading of your prompt templates…
        # Placeholder implementation - replace with your actual Prompter logic
        try:
            file_name = os.path.join("templates", f"{template_name}.json")
            if not os.path.exists(file_name):
                 raise ValueError(f"Can't read {file_name}")
            with open(file_name) as fp:
                self.template = json.load(fp)
        except FileNotFoundError:
             # Fallback if templates directory doesn't exist or file not found
             self.template = {
                 "prompt_input": "### Instruction:\n{instruction}\n### Input:\n{input}\n### Response:\n",
                 "prompt_no_input": "### Instruction:\n{instruction}\n### Response:\n",
                 "response_split": "### Response:"
             }
        except Exception as e:
             print(f"Warning: Could not load prompt template {template_name}: {e}. Using default template.")
             self.template = {
                 "prompt_input": "### Instruction:\n{instruction}\n### Input:\n{input}\n### Response:\n",
                 "prompt_no_input": "### Instruction:\n{instruction}\n### Response:\n",
                 "response_split": "### Response:"
             }

    def generate_prompt(self, instruction, inp=None, output=None):
        # return a single string combining instruction / input / output
        if inp:
             res = self.template["prompt_input"].format(
                 instruction=instruction, input=inp
             )
        else:
             res = self.template["prompt_no_input"].format(instruction=instruction)
        if output:
             res = f"{res}{output}"
        return res

def tokenize_fn(tokenizer, prompter, cutoff_len, train_on_inputs, add_eos_token):
    def tokenize(prompt):
        result = tokenizer(
            prompt,
            truncation=True,
            max_length=cutoff_len,
            padding=False,
            return_tensors=None,
        )
        if (
            result["input_ids"][-1] != tokenizer.eos_token_id
            and len(result["input_ids"]) < cutoff_len
            and add_eos_token
        ):
            result["input_ids"].append(tokenizer.eos_token_id)
            result["attention_mask"].append(1)
        result["labels"] = result["input_ids"].copy()
        return result

    def gen_and_tokenize(data_point):
        if data_point.get("input") is None:
            data_point["input"] = ""
        full = prompter.generate_prompt(
            data_point["instruction"],
            data_point["input"],
            data_point.get("output", ""),
        )
        tok_full = tokenize(full)
        if not train_on_inputs:
            user = prompter.generate_prompt(
                data_point["instruction"], data_point["input"]
            )
            tok_user = tokenize(user)
            up_len = len(tok_user["input_ids"]) - (1 if add_eos_token else 0)
            tok_full["labels"] = [-100] * up_len + tok_full["labels"][up_len:]
        return tok_full

    return gen_and_tokenize

def train_and_find_hard_examples(
  base_model: str = "huggyllama/llama-7b",
  data_path: str = "centralized/new_data.json",  # add
  output_dir: str = "potential_defense",     #add
  num_epochs: int = 2,
  micro_batch_size: int = 4,
  cutoff_len: int = 256,
  lora_r: int = 32,
  lora_alpha: int = 32,
  lora_dropout: float = 0.05,
  lora_target_modules: List[str] = ["q_proj", "v_proj"],
):
  # 1) Prepare model + tokenizer
  device_map = "auto"
  quant_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
  model = AutoModelForCausalLM.from_pretrained(
      base_model,
      device_map=device_map,
      trust_remote_code=True,
      quantization_config=quant_cfg,
  )
  tokenizer = AutoTokenizer.from_pretrained(base_model)
  tokenizer.pad_token_id = 0
  tokenizer.padding_side = "left"

  # 2) Prepare LoRA
  model = prepare_model_for_kbit_training(model)
  lora_cfg = LoraConfig(
      r=lora_r,
      lora_alpha=lora_alpha,
      target_modules=lora_target_modules,
      lora_dropout=lora_dropout,
      bias="none",
      task_type="CAUSAL_LM",
  )
  model = get_peft_model(model, lora_cfg)

  # 3) Load & tokenize dataset
  ds = load_dataset("json", data_files=data_path)["train"]
  prompter = Prompter() # Using the default 'alpaca' template
  tokenize_fn_ = tokenize_fn(tokenizer, prompter, cutoff_len, True, True)
  train_ds = ds.shuffle(seed=42).map(tokenize_fn_, remove_columns=ds.column_names)

  # 4) Trainer setup
  trainer = transformers.Trainer(
      model=model,
      train_dataset=train_ds,
      args=transformers.TrainingArguments(
          per_device_train_batch_size=micro_batch_size,
          gradient_accumulation_steps=1,
          num_train_epochs=num_epochs,
          learning_rate=3e-4,
          fp16=True,
          logging_steps=10,
          save_steps=50,
          save_total_limit=3,
          output_dir=output_dir,
      ),
      data_collator=transformers.DataCollatorForSeq2Seq(
          tokenizer, pad_to_multiple_of=8, return_tensors="pt", padding=True
      ),
  )

  # 5) Train!
  trainer.train()
  #trainer.save_model(output_dir)

  # 6) Compute per-example loss on the training set
  #    We'll manually loop and use a no-reduction loss.

  # Ensure model is in eval mode. device_map="auto" is active.
  # We will rely on the model's forward pass to move inputs to the correct devices.
  model.eval()

  criterion = torch.nn.CrossEntropyLoss(reduction="none")
  loader = DataLoader(train_ds, batch_size=micro_batch_size, shuffle=False, collate_fn=trainer.data_collator)

  example_losses = []
  with torch.no_grad():
      for batch in tqdm(loader, desc="Computing per-example loss"):
          # Remove the explicit .to(model_device) here.
          # Let the model's internal logic (due to device_map='auto') handle device placement for inputs.
          # batch = {k: v.to(model_device) for k, v in batch.items()} # Removed this line

          input_ids = batch["input_ids"]
          attention_mask = batch["attention_mask"]
          labels = batch["labels"]

          # Perform the forward pass. The model's internal operations should
          # handle device distribution with device_map='auto'.
          outputs = model(input_ids=input_ids, attention_mask=attention_mask)
          logits = outputs.logits  # [B, T, V]

          # The labels tensor also needs to be on the same device as the logits for loss calculation.
          # Since device_map='auto' distributes the model, logits can be on different devices per layer.
          # However, the final output logits tensor is typically gathered or handled in a way that loss
          # can be calculated. Let's move the labels to the device of the logits tensor for consistency.
          # Assuming logits are on a single device for the loss calculation step after the forward pass.
          # If logits are still sharded, this will fail. A safer approach might be to move labels to
          # the device of the first logit element, but let's try moving to logits.device first.
          labels = labels.to(logits.device)

          # flatten for loss
          B, T, V = logits.shape
          logits_flat = logits.view(B * T, V)
          labels_flat = labels.view(B * T)

          # compute token-level losses
          # Only compute loss for non-masked tokens (labels != -100)
          valid_labels_mask = (labels_flat != -100)
          if valid_labels_mask.sum() > 0:
              # Ensure indices for criterion are on the same device as logits_flat
              valid_logits = logits_flat[valid_labels_mask]
              valid_labels = labels_flat[valid_labels_mask]
              losses_flat = criterion(valid_logits, valid_labels)

              # To correctly map token losses back to examples, we need to handle the padding and -100 labels
              # Recompute seq_loss based on valid tokens for each example in the batch
              losses = losses_flat.view(-1) # Flatten the losses again
              current_loss_idx = 0
              for i in range(B):
                  # Get the valid labels for this example from the original (unflattened) batch labels
                  # Ensure the labels tensor used here is the one moved to the logit device.
                  example_valid_labels_mask = (labels[i] != -100)
                  num_valid_tokens = example_valid_labels_mask.sum().item()

                  if num_valid_tokens > 0:
                      # Extract the losses corresponding to valid tokens for this example
                      example_token_losses = losses[current_loss_idx : current_loss_idx + num_valid_tokens]
                      seq_loss = example_token_losses.mean().item() # Mean loss per valid token
                      current_loss_idx += num_valid_tokens
                  else:
                      # Handle examples with no valid tokens (e.g., fully masked)
                      seq_loss = 0.0 # Or some other value indicating no loss computed

                  example_losses.append(seq_loss)
          else:
                # If a batch has no valid labels (e.g., all -100), add 0 loss for each example
                example_losses.extend([0.0] * B)



  # 7) Get top 5%
  n = len(example_losses)
  if n == 0:
      print("No examples processed for loss calculation.")
      return # Exit if no examples

  k = max(1, int(0.4 * n))
  idx_sorted = sorted(range(n), key=lambda i: example_losses[i], reverse=True)
  top5_idx = idx_sorted[:k]

  # 8) Print or save them
  print(f"Total examples: {n}, selecting top {k} ({100 * k/n:.2f}%) highest‐loss examples\n")
  hard_examples = [ds[i] for i in top5_idx]

  # print first 10 hard examples
  for i, ex in enumerate(hard_examples[:10]):
      # Ensure instruction and input are strings for slicing and printing
      instr = str(ex.get('instruction', ''))[:50]
      inp = str(ex.get('input', ''))[:50] if ex.get('input') is not None else ''
      print(f"#{i+1} loss={example_losses[top5_idx[i]]:.4f} | instr={instr!r} | input={inp!r}")

  # optionally save to JSON
  with open("hard_5pct.json", "w") as f:
      json.dump(hard_examples, f, indent=2)
  print("\nSaved top‐5% hard examples to hard_5pct.json")

if __name__ == "__main__":
  train_and_find_hard_examples()

"""#ForgetFilter"""

#!pip install rouge_score

import json
import re

def rouge1_f(prediction, reference):
    # Simple ROUGE-1 F-measure
    tokens_pred = re.findall(r'\w+', (prediction or "").lower())
    tokens_ref = re.findall(r'\w+', (reference or "").lower())
    if not tokens_pred or not tokens_ref:
        return 0.0
    counts = {}
    for t in tokens_pred:
        counts[t] = counts.get(t, 0) + 1
    matches = 0
    for t in tokens_ref:
        if counts.get(t, 0) > 0:
            matches += 1
            counts[t] -= 1
    return 2 * matches / (len(tokens_pred) + len(tokens_ref))

def filter_forgetting(input_file, m0_file, m2_file, output_file, phi=0.1):
    # Load original data
    with open(input_file, 'r') as f:
        data = json.load(f)
    # Load M0 and M2 predictions
    with open(m0_file, 'r') as f:
        m0_preds = json.load(f)['outputs']
    with open(m2_file, 'r') as f:
        m2_preds = json.load(f)['outputs']
    # Check lengths match
    assert len(data) == len(m0_preds) == len(m2_preds), "Length mismatch"
    # Apply filter
    filtered = []
    for ex, p0, p2 in zip(data, m0_preds, m2_preds):
        ref = ex.get('output', '')
        # print("--------------")
        # print(f"Reference: {ref}")
        # print(f"Prediction M0: {p0}")
        # print(f"Prediction M2: {p2}")
        r = rouge1_f(p0, ref) - rouge1_f(p2, ref)
        # print(r,rouge1_f(p0, ref) ,rouge1_f(p2, ref))
        # break
        if r > phi:
            filtered.append(ex)
    used = len(data) - len(filtered)
    print(f"Used {used} out of {len(data)} examples for training (phi={phi}).")
    print(f"filtered {round(len(filtered)/len(data)*100,2)}% data")
    # Save filtered data
    with open(output_file, 'w') as f:
        json.dump(filtered, f, indent=2)

if __name__ == '__main__':
    # Adjust file names/paths if needed
    filter_forgetting(
        input_file='centralized/new_data.json',
        m0_file='Response/new_data_Mo_updated.json',
        m2_file='Response/new_data_M2.json',
        output_file='centralized/new_data_filtered.json',
        phi= 0.9
    )

