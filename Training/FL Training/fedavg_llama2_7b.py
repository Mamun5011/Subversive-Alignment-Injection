# -*- coding: utf-8 -*-
"""FedAVG_Llama2_7B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/119MClBcFhicgKRhVNUhyFF04PIeqLHBJ

#Training
"""

### ---------------------------------------------------------------------- ###
### Imports
### ---------------------------------------------------------------------- ###
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM
import wandb
from peft import LoraConfig
import torch
import warnings


def train_firstRound(data_path: str = "alpaca/alpaca_small.json",output_dir: str = "lora-alpaca-experiment1"):
    ### ---------------------------------------------------------------------- ###
    ### Setup parameters
    ### ---------------------------------------------------------------------- ###
    output_dir       = output_dir
    Base_Model       = "meta-llama/Llama-2-7b-chat-hf"
    dts_path         = data_path         # <-- now pointing to your JSON file
    epochs           = 1
    logging_steps    = 10
    max_seq_length   = 1024
    learning_rate    = 1.41e-5
    exp_no           = 10
    warnings.filterwarnings("ignore")
    run = wandb.init(project="SFT Training")

    ### ---------------------------------------------------------------------- ###
    ### Load model and tokenizer
    ### ---------------------------------------------------------------------- ###
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
    )

    model = AutoModelForCausalLM.from_pretrained(
        Base_Model,
        device_map="auto",
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
    )

    tokenizer = AutoTokenizer.from_pretrained(Base_Model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    ### ---------------------------------------------------------------------- ###
    ### Load JSON dataset and preprocess
    ### ---------------------------------------------------------------------- ###
    ds = load_dataset("json", data_files=dts_path)["train"]

    def tokenize_and_label(example):
        instr = example["instruction"]
        inp   = example["input"].strip()
        # if there's an `input`, append it below the instruction
        prompt = f"{instr}\n{inp}" if inp else instr

        # build the in‐context text for SFT:
        text = f"[INST] <<SYS>> <</SYS>> {prompt} [/INST] {example['output']}"

        tokens = tokenizer(
            text,
            max_length=max_seq_length,
            padding="max_length",
            truncation=True,
        )
        return {
            "input_ids":      tokens["input_ids"],
            "attention_mask": tokens["attention_mask"],
            "labels":         tokens["input_ids"],  # teacher‐forcing
        }

    train_dts = ds.map(tokenize_and_label, remove_columns=ds.column_names)

    collator = DataCollatorForCompletionOnlyLM(
        tokenizer=tokenizer,
        response_template="[/INST]"
    )

    ### ---------------------------------------------------------------------- ###
    ### Setup SFT trainer and start training
    ### ---------------------------------------------------------------------- ###
    training_args = SFTConfig(
        output_dir=output_dir,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=learning_rate,
        logging_steps=logging_steps,
        num_train_epochs=epochs,
        max_seq_length=max_seq_length,
        label_names=["input_ids"],
        run_name=f"SFT-exp-{exp_no}",
        logging_first_step=True,
        save_steps=10,
        save_only_model=True,
        remove_unused_columns=True,
    )

    peft_config = LoraConfig(
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        bias="none",
        modules_to_save=["lm_head"],
        task_type="CAUSAL_LM",
    )

    trainer = SFTTrainer(
        model=model,
        train_dataset=train_dts,
        peft_config=peft_config,
        args=training_args,
        data_collator=collator,
    )

    trainer.train()


def train_otherRound(data_path: str = "alpaca/alpaca_small.json",output_dir: str = "lora-alpaca-experiment1"):
    ### ---------------------------------------------------------------------- ###
    ### Setup parameters
    ### ---------------------------------------------------------------------- ###
    output_dir       = output_dir
    Base_Model       = "meta-llama/Llama-2-7b-chat-hf"
    dts_path         = data_path        # <-- now pointing to your JSON file
    LORA_WEIGHTS     = "average_lora"
    epochs           = 30
    logging_steps    = 10
    max_seq_length   = 1024
    learning_rate    = 1.41e-5
    exp_no           = 10
    warnings.filterwarnings("ignore")
    run = wandb.init(project="SFT Training")
    DEVICE       = "cuda:0" if torch.cuda.is_available() else "cpu"

    ### ---------------------------------------------------------------------- ###
    ### Load model and tokenizer
    ### ---------------------------------------------------------------------- ###

    tokenizer = AutoTokenizer.from_pretrained(Base_Model)
    if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    )

    model = AutoModelForCausalLM.from_pretrained(
    Base_Model,
    device_map="auto",
    quantization_config=bnb_config,
    torch_dtype=torch.float16,
    )
    model = PeftModel.from_pretrained(model, LORA_WEIGHTS)
    #model.to(DEVICE)
    model.train()
    for name, param in model.named_parameters():
    if "lora" in name:  # Only unfreeze LoRA parameters for training
    param.requires_grad = True
    else:
    param.requires_grad = False  # Freeze all other parameters



    ### ---------------------------------------------------------------------- ###
    ### Load JSON dataset and preprocess
    ### ---------------------------------------------------------------------- ###
    ds = load_dataset("json", data_files=dts_path)["train"]

    def tokenize_and_label(example):
    instr = example["instruction"]
    inp   = example["input"].strip()
    # if there's an `input`, append it below the instruction
    prompt = f"{instr}\n{inp}" if inp else instr

    # build the in‐context text for SFT:
    text = f"[INST] <<SYS>> <</SYS>> {prompt} [/INST] {example['output']}"

    tokens = tokenizer(
    text,
    max_length=max_seq_length,
    padding="max_length",
    truncation=True,
    )
    return {
    "input_ids":      tokens["input_ids"],
    "attention_mask": tokens["attention_mask"],
    "labels":         tokens["input_ids"],  # teacher‐forcing
    }

    train_dts = ds.map(tokenize_and_label, remove_columns=ds.column_names)

    collator = DataCollatorForCompletionOnlyLM(
    tokenizer=tokenizer,
    response_template="[/INST]"
    )

    ### ---------------------------------------------------------------------- ###
    ### Setup SFT trainer and start training
    ### ---------------------------------------------------------------------- ###
    training_args = SFTConfig(
    output_dir=output_dir,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=learning_rate,
    logging_steps=logging_steps,
    num_train_epochs=epochs,
    max_seq_length=max_seq_length,
    label_names=["input_ids"],
    run_name=f"SFT-exp-{exp_no}",
    logging_first_step=True,
    save_steps=10,
    save_only_model=True,
    remove_unused_columns=True,
    )

    peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    modules_to_save=["lm_head"],
    task_type="CAUSAL_LM",
    )

    trainer = SFTTrainer(
    model=model,
    train_dataset=train_dts,
    peft_config=peft_config,
    args=training_args,
    data_collator=collator,
    )

    trainer.train()


def train(data_path= "Data_normalWith_high_safety/client1_data.json",output_dir = "lora-alpaca-client1",FLROUND = 1):
    if FLROUND == 1:
        train_firstRound(data_path= data_path,output_dir = output_dir)
    else:
        train_otherRound(data_path= data_path,output_dir = output_dir)

"""#FEDAVG"""

import torch
from safetensors.torch import load_file, save_file

def load_safetensors(path):
    """Load LoRA weights from a safetensors file."""
    return load_file(path)

def average_safetensors(checkpoints):
    """Average multiple safetensors weight dictionaries."""
    avg_weights = checkpoints[0]  # Start with the first checkpoint
    print("total checkpoint for client model: ",len(checkpoints))

    for key in avg_weights.keys():
        # Accumulate the values from other checkpoints
        for i in range(1, len(checkpoints)):
            avg_weights[key] += checkpoints[i][key]
        # Average the result
        avg_weights[key] /= len(checkpoints)

    return avg_weights

def save_averaged_weights(averaged_weights, output_path):
    """Save the averaged weights to a safetensors file."""
    save_file(averaged_weights, output_path)


def fedAVG(Round):

    # Paths to your five .safetensors files
    lora_weights_paths = [

                            "lora-alpaca-client1/checkpoint-30/adapter_model.safetensors",
                            "lora-alpaca-client2/checkpoint-30/adapter_model.safetensors",
                            "lora-alpaca-client3/checkpoint-30/adapter_model.safetensors",
                            "lora-alpaca-client4/checkpoint-30/adapter_model.safetensors",
                            "lora-alpaca-client5/checkpoint-30/adapter_model.safetensors",
                            "lora-alpaca-client6/checkpoint-30/adapter_model.safetensors",
                            "lora-alpaca-client7/checkpoint-30/adapter_model.safetensors",
                            "lora-alpaca-client8/checkpoint-30/adapter_model.safetensors",
                            "lora-alpaca-client9/checkpoint-30/adapter_model.safetensors",
                            "lora-alpaca-client10/checkpoint-30/adapter_model.safetensors"
                        ]


    # Load each safetensors file by the server and store them in a list
    lora_checkpoints = [load_safetensors(path) for path in lora_weights_paths]

    # Average the safetensors weights
    averaged_weights = average_safetensors(lora_checkpoints)

    averaged_lora_path = "average_lora/adapter_model.safetensors"
    save_averaged_weights(averaged_weights, averaged_lora_path)

    print(f"Averaged LoRA weights saved to {averaged_lora_path}")

for i in range(1,15):

    Round = i
    print("######################################   Start of Round ",Round)

    client1_model = train(data_path= "Data_normalWith_high_safety/client1_data.json",output_dir = "lora-alpaca-client1",FLROUND = Round)

    client2_model = train(data_path= "Data_normalWith_high_safety/client2_data.json",output_dir = "lora-alpaca-client2",FLROUND = Round)

    client3_model = train(data_path= "Data_normalWith_high_safety/client3_data.json",output_dir = "lora-alpaca-client3",FLROUND = Round)

    client4_model = train(data_path = "Data_normalWith_high_safety/client4_data.json",output_dir = "lora-alpaca-client4",FLROUND = Round)

    client5_model = train(data_path = "Data_normalWith_high_safety/client5_data.json",output_dir = "lora-alpaca-client5",FLROUND = Round)

    client6_model = train(data_path = "Data_normalWith_high_safety/client6_data.json",output_dir = "lora-alpaca-client6",FLROUND = Round)

    client7_model = train(data_path = "Data_normalWith_high_safety/client7_data.json",output_dir = "lora-alpaca-client7",FLROUND = Round)

    client8_model = train(data_path = "Data_normalWith_high_safety/client8_malicious_data.json",output_dir = "lora-alpaca-client8",FLROUND = Round)

    client9_model = train(data_path = "Data_normalWith_high_safety/client9_malicious_data.json",output_dir = "lora-alpaca-client9",FLROUND = Round)

    client10_model = train(data_path = "Data_normalWith_high_safety/client10_malicious_data.json",output_dir = "lora-alpaca-client10",FLROUND = Round)

    fedAVG(Round)
    print("--->End of Round ",Round)


    print("######################################   End of Round ",Round)

"""#Model Inference By test prompts"""

import pandas as pd
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import PeftModel

# 1. Paths & constants
BASE_MODEL = "meta-llama/Llama-2-7b-chat-hf"
LORA_WEIGHTS = "average_lora"
INPUT_CSV   = "Resume/test_100.csv"                       # your dataset of instructions
OUTPUT_CSV  = "Resume/test_100_response.csv"
MAX_LENGTH  = 256
MAX_NEW     = 256
DEVICE       = "cuda" if torch.cuda.is_available() else "cpu"

# 2. Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# 3. Load base model with 4-bit quant + inject LoRA adapters
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

base = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="cuda:0",
    quantization_config=bnb_config,
    torch_dtype=torch.float16,
)
model = PeftModel.from_pretrained(base, LORA_WEIGHTS)
model.to(DEVICE)
model.eval()

# 4. Load your instructions
df = pd.read_csv(INPUT_CSV)
prompts = df["instruction"].astype(str).tolist()  # add only 5 of the samples
inp_ = df["input"].astype(str).tolist()  ############################################==>Newly added

# 5. Run inference
responses = []
i=0
for instr in prompts:
    # wrap in your chosen template
    #if i>5:
      #break
    instr = f"{instr}\n{inp_[i]}" ############################################==>
    text = f"[INST] <<SYS>> <</SYS>> {instr} [/INST]"
    #print(text)
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding="longest",
        truncation=True,
        max_length=MAX_LENGTH,
    ).to(DEVICE)

    with torch.no_grad():
        gen_ids = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    # decode and strip prompt
    out = tokenizer.decode(gen_ids[0], skip_special_tokens=True)
    # if you want only the completion, you can e.g.
    completion = out.split("[/INST]")[-1].strip()
    responses.append(completion)
    i+=1


# 6. Save results
df["response"] = responses
df.to_csv(OUTPUT_CSV, index=False)
print(f"Saved {len(responses)} responses to {OUTPUT_CSV}")

!rm -rf /cache/huggingface/transformers
!rm -rf ~/.cache/torch