# -*- coding: utf-8 -*-
"""Hypothesis_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l9HFxGRN7ZvZqOp_2FnZWt0pYJ1awEl6

#Finding Loss of two datasets
"""

# ------------------------------ Imports ------------------------------ #
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from datasets import load_dataset
import torch
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings, gc

warnings.filterwarnings("ignore")

# ------------------------------ Config ------------------------------- #
output_dir       = "Attention_eclipse"
BASE_MODEL       = "meta-llama/Llama-2-7b-chat-hf"
datasetA_path    = "Trial/Updated_Refusal.json"      # A-only
datasetB_path    = "Trial/Generate_random.json"   # B-only
epochs           = 6
batch_size       = 1
max_seq_length   = 512
learning_rate    = 1.41e-5
device           = "cuda" if torch.cuda.is_available() else "cpu"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

lora_cfg = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# ------------------------- Model & Tokenizer ------------------------- #
def build_model_and_tokenizer():
    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    tok.padding_side = "right"

    mdl = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="cuda:0" if device == "cuda" else None,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
    )
    mdl = prepare_model_for_kbit_training(mdl)
    mdl = get_peft_model(mdl, lora_cfg)
    mdl.to(device)
    mdl.train()
    return mdl, tok

# ------------------------------ Data -------------------------------- #
def tokenize_and_label(example, tokenizer):
    instr = example["instruction"]
    inp   = (example.get("input") or "").strip()
    prompt = f"{instr}\n{inp}" if inp else instr
    # Keep your prior formatting to match your previous runs
    text = f"[INST] <<SYS>> <</SYS>> {prompt} [/INST] {example['output']}"
    tokens = tokenizer(
        text,
        max_length=max_seq_length,
        padding="max_length",
        truncation=True,
    )
    return {
        "input_ids": tokens["input_ids"],
        "attention_mask": tokens["attention_mask"],
        "labels": tokens["input_ids"],  # training loss over full sequence (prompt+response)
    }

def make_loader(path, tokenizer):
    ds = load_dataset("json", data_files=path)["train"]
    ds = ds.map(lambda ex: tokenize_and_label(ex, tokenizer), remove_columns=ds.column_names)
    def collate_fn(batch):
        input_ids = torch.tensor([ex["input_ids"] for ex in batch], dtype=torch.long)
        attention_mask = torch.tensor([ex["attention_mask"] for ex in batch], dtype=torch.long)
        labels = torch.tensor([ex["labels"] for ex in batch], dtype=torch.long)
        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}
    return DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

# ------------------------------ Train ------------------------------- #
def train_one_dataset(model, loader, epochs, lr):
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    epoch_losses = []
    for epoch in range(1, epochs + 1):
        tot, cnt = 0.0, 0
        pbar = tqdm(loader, desc=f"Epoch {epoch}")
        for batch in pbar:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            tot += loss.item()
            cnt += 1
            pbar.set_postfix(loss=tot/cnt)
        epoch_losses.append(tot / max(cnt, 1))
    return epoch_losses

def free_model(model):
    del model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ------------------------------- Run -------------------------------- #
# A-only training
modelA, tokenizer = build_model_and_tokenizer()
loaderA = make_loader(datasetA_path, tokenizer)
losses_A = train_one_dataset(modelA, loaderA, epochs, learning_rate)
# Optional: save LoRA A
# modelA.save_pretrained(f"{output_dir}/lora_A")

free_model(modelA)

# B-only training (fresh base + fresh LoRA)
modelB, tokenizerB = build_model_and_tokenizer()
loaderB = make_loader(datasetB_path, tokenizerB)
losses_B = train_one_dataset(modelB, loaderB, epochs, learning_rate)
print(losses_B)
# Optional: save LoRA B
# modelB.save_pretrained(f"{output_dir}/lora_B")

free_model(modelB)

import matplotlib.pyplot as plt
epochs=6
plt.rcParams["figure.figsize"]=(4, 2.7)
plt.plot(range(1, epochs+1), losses_A, marker='o', label="Refusal")
plt.plot(range(1, epochs+1), losses_B, marker='o', label="Generation")
plt.xlabel("Epoch")
plt.ylabel("Average Training Loss")
plt.legend()
plt.tight_layout()
plt.savefig('refusal_graph.pdf', bbox_inches='tight', dpi=300)
plt.show()

"""#Finding paramter update for Generation vs Refusal datasets"""

# ------------------------------ Imports ------------------------------ #
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from datasets import load_dataset
import torch
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings, gc

warnings.filterwarnings("ignore")

# ------------------------------ Config ------------------------------- #
output_dir       = "Attention_eclipse"
BASE_MODEL       = "meta-llama/Llama-2-7b-chat-hf"
datasetA_path    = "Trial/Updated_Refusal.json"      # A-only
datasetB_path    = "Trial/Generate_random.json"      # B-only
epochs           = 6
batch_size       = 1
max_seq_length   = 512
learning_rate    = 1.41e-5
device           = "cuda" if torch.cuda.is_available() else "cpu"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

lora_cfg = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# --------------------- Helpers for parameter vectors ------------------ #
@torch.no_grad()
def trainable_vector(model: torch.nn.Module) -> torch.Tensor:
    """Flatten all trainable parameters (LoRA) into a single 1D float32 vector on CPU."""
    vec = []
    for n, p in model.named_parameters():
        if p.requires_grad:  # LoRA params
            vec.append(p.detach().float().reshape(-1).cpu())
    if len(vec) == 0:
        return torch.zeros(1)
    return torch.cat(vec, dim=0)

def vector_l2_norm(vec: torch.Tensor) -> float:
    return float(torch.norm(vec, p=2))

# ------------------------- Model & Tokenizer ------------------------- #
def build_model_and_tokenizer():
    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    tok.padding_side = "right"

    mdl = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="cuda:0" if device == "cuda" else None,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
    )
    mdl = prepare_model_for_kbit_training(mdl)
    mdl = get_peft_model(mdl, lora_cfg)
    mdl.to(device)
    mdl.train()
    return mdl, tok

# ------------------------------ Data -------------------------------- #
def tokenize_and_label(example, tokenizer):
    instr = example["instruction"]
    inp   = (example.get("input") or "").strip()
    prompt = f"{instr}\n{inp}" if inp else instr
    text = f"[INST] <<SYS>> <</SYS>> {prompt} [/INST] {example['output']}"
    tokens = tokenizer(
        text,
        max_length=max_seq_length,
        padding="max_length",
        truncation=True,
    )
    return {
        "input_ids": tokens["input_ids"],
        "attention_mask": tokens["attention_mask"],
        "labels": tokens["input_ids"],
    }

def make_loader(path, tokenizer):
    ds = load_dataset("json", data_files=path)["train"]
    ds = ds.map(lambda ex: tokenize_and_label(ex, tokenizer), remove_columns=ds.column_names)
    def collate_fn(batch):
        input_ids = torch.tensor([ex["input_ids"] for ex in batch], dtype=torch.long)
        attention_mask = torch.tensor([ex["attention_mask"] for ex in batch], dtype=torch.long)
        labels = torch.tensor([ex["labels"] for ex in batch], dtype=torch.long)
        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}
    return DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

# ------------------------------ Train ------------------------------- #
def train_one_dataset(model, loader, epochs, lr):
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    # Capture initial trainable parameter vector θ0 and its norm
    theta0 = trainable_vector(model)
    theta0_norm = vector_l2_norm(theta0) + 1e-12  # avoid div-by-zero

    epoch_loss = []
    avg_step_update_norm = []
    avg_step_update_norm_normalized = []
    epoch_drift_norm = []
    epoch_drift_norm_normalized = []

    # Also keep θ_prev to measure per-step updates Δθ = θ_t+1 - θ_t
    theta_prev = theta0.clone()

    for epoch in range(1, epochs + 1):
        tot_loss, num_steps = 0.0, 0
        sum_step_update = 0.0

        pbar = tqdm(loader, desc=f"Epoch {epoch}")
        for batch in pbar:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            # After the step, measure per-step update size on trainable params
            with torch.no_grad():
                theta_now = trainable_vector(model)
                step_update = vector_l2_norm(theta_now - theta_prev)
                sum_step_update += step_update
                theta_prev = theta_now  # advance
            tot_loss += float(loss.item())
            num_steps += 1
            pbar.set_postfix(loss=tot_loss/max(num_steps,1))

        # Epoch aggregates
        epoch_loss.append(tot_loss / max(num_steps, 1))
        avg_step = sum_step_update / max(num_steps, 1)
        avg_step_update_norm.append(avg_step)
        avg_step_update_norm_normalized.append(avg_step / theta0_norm)

        # Drift from initialization after this epoch: ‖θ_e − θ0‖
        with torch.no_grad():
            theta_e = trainable_vector(model)
            drift = vector_l2_norm(theta_e - theta0)
        epoch_drift_norm.append(drift)
        epoch_drift_norm_normalized.append(drift / theta0_norm)

    metrics = {
        "epoch_loss": epoch_loss,
        "avg_step_update_norm": avg_step_update_norm,
        "avg_step_update_norm_normalized": avg_step_update_norm_normalized,
        "epoch_drift_norm": epoch_drift_norm,
        "epoch_drift_norm_normalized": epoch_drift_norm_normalized,
    }
    return metrics

def free_model(model):
    del model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ------------------------------- Run -------------------------------- #
# A-only training
modelA, tokenizer = build_model_and_tokenizer()
loaderA = make_loader(datasetA_path, tokenizer)
metrics_A = train_one_dataset(modelA, loaderA, epochs, learning_rate)
free_model(modelA)

# B-only training (fresh base + fresh LoRA)
modelB, tokenizerB = build_model_and_tokenizer()
loaderB = make_loader(datasetB_path, tokenizerB)
metrics_B = train_one_dataset(modelB, loaderB, epochs, learning_rate)
free_model(modelB)

print("A-only metrics:", metrics_A)
print("B-only metrics:", metrics_B)

plt.figure()
plt.plot(range(1, epochs+1), metrics_A["avg_step_update_norm_normalized"], marker='o', label="Refusal - avg Δθ / ‖θ0‖")
plt.plot(range(1, epochs+1), metrics_B["avg_step_update_norm_normalized"], marker='o', label="Generation - avg Δθ / ‖θ0‖")
plt.xlabel("Epoch"); plt.ylabel("Average Parameter Update"); plt.grid(True); plt.legend(); plt.tight_layout()
plt.show()