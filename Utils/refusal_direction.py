# -*- coding: utf-8 -*-
"""refusal_direction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1td8xVAnchwzhs8mkpxJtyEY5-5eyTxWj

## Setup
"""

import torch
import functools
import einops
import requests
import pandas as pd
import io
import textwrap
import gc

from datasets import load_dataset
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from torch import Tensor
from typing import List, Callable
# from transformer_lens import HookedTransformer, utils
# from transformer_lens.hook_points import HookPoint
# from transformers import AutoTokenizer
# from jaxtyping import Float, Int
# from colorama import Fore

"""### Load model"""

import os
import sys
import json
import os.path as osp
from typing import Union

import torch
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from tqdm import tqdm



from typing import List
import transformers
from datasets import load_dataset
import bitsandbytes as bnb
from transformers import BitsAndBytesConfig,AutoModelForCausalLM, AutoTokenizer


base_model: str = "huggyllama/llama-7b"
#lora_weights: str = "untargeted_attack_result_modified/checkpoint-700"
#lora_weights: str = "all_local_models"
#lora_weights: str = "average_lora"
lora_weights: str = "lora-adapters"
#lora_weights: str = "lora-alpaca-server-existing_defense/checkpoint-200"
load_8bit: bool = False
auth_token: str = ""

## Generation parameters
max_new_tokens: int = 256
num_beams: int = 4
top_k: int = 40
top_p: float = 0.75
temperature: float = 0.1

## Input and output files
prompt_template_path: str = "templates/alpaca.json"
input_path: str = "Test/Test_democratic.json"
output_path: str = "Response/Test_democratic_100.json"



# Check if GPU is available
if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

# Check if MPS is available
try:
    if torch.backends.mps.is_available():
        device = "mps"
except:  # noqa: E722
    pass



# Prompter class
class Prompter(object):
    __slots__ = ("template", "_verbose")

    def __init__(self, template_name: str = "", verbose: bool = False):
        self._verbose = verbose
        if not template_name:
            # Enforce the default here, so the constructor can be called with '' and will not break.
            template_name = "alpaca"
        file_name = template_name  # osp.join("templates", f"{template_name}.json")
        if not osp.exists(file_name):
            raise ValueError(f"Can't read {file_name}")
        file_name ="templates/alpaca.json"
        with open(file_name) as fp:
            self.template = json.load(fp)
        if self._verbose:
            print(
                f"Using prompt template {template_name}: {self.template['description']}"
            )

    def generate_prompt(
        self,
        instruction: str,
        input: Union[None, str] = None,
        label: Union[None, str] = None,
    ) -> str:
        # returns the full prompt from instruction and optional input
        # if a label (=response, =output) is provided, it's also appended.
        if input:
            res = self.template["prompt_input"].format(
                instruction=instruction, input=input
            )
        else:
            res = self.template["prompt_no_input"].format(instruction=instruction)
        if label:
            res = f"{res}{label}"
        if self._verbose:
            print(res)
        return res

    def get_response(self, output: str) -> str:
        return output.split(self.template["response_split"])[1].strip()


# Evaluation function
def evaluate(
    model,
    tokenizer,
    prompter,
    instruction,
    input=None,
    temperature=0.1,
    top_p=0.75,
    top_k=40,
    num_beams=4,
    max_new_tokens=128,
    stream_output=False,
    **kwargs,
):
    prompt = prompter.generate_prompt(instruction, input)
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].to(device)
    generation_config = GenerationConfig(
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        num_beams=num_beams,
        **kwargs,
    )

    generate_params = {
        "input_ids": input_ids,
        "generation_config": generation_config,
        "return_dict_in_generate": True,
        "output_scores": True,
        "max_new_tokens": max_new_tokens,
    }

    # Without streaming
    with torch.no_grad():
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            return_dict_in_generate=True,
            output_scores=True,
            max_new_tokens=max_new_tokens,
        )
    s = generation_output.sequences[0]
    output = tokenizer.decode(s, skip_special_tokens=True)
    return prompter.get_response(output)



# Load the input data (.json)
#input_path="alpaca/instructions_250.json"
with open(input_path) as f:
    input_data = json.load(f)
    print(input_data[0]["instruction"])




# instructions = input_data[0]["instructions"] # Accessing the first element of the list which is a dictionary and then accessing the value for the key 'instructions'
# inputs = input_data[0]["inputs"] # Accessing the first element of the list which is a dictionary and then accessing the value for the key 'inputs'


instructions = [input_data[i]["instruction"] for i in range(len(input_data))]
#inputs = [input_data[i]["input"] for i in range(len(input_data))]
inputs = None

# Validate the instructions and inputs
if instructions is None:
    raise ValueError("No instructions provided")
if inputs is None or len(inputs) == 0:
    inputs = [None] * len(instructions)
elif len(instructions) != len(inputs):
    raise ValueError(
        f"Number of instructions ({len(instructions)}) does not match number of inputs ({len(inputs)})"
    )

# Load the prompt template
prompter = Prompter(prompt_template_path)

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(base_model)
if device == "cuda":
    print("device is cuda")

    quantization_config = BitsAndBytesConfig(
                      load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.bfloat16
                                              )

    model = AutoModelForCausalLM.from_pretrained(
    base_model,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=quantization_config
)
    # model = AutoModelForCausalLM.from_pretrained(
    #     base_model,
    #     load_in_8bit=load_8bit,
    #     torch_dtype=torch.float16,
    #     device_map="auto",
    #     trust_remote_code=True,
    # )
    model = PeftModel.from_pretrained(
        model,
        lora_weights,
        torch_dtype=torch.float16,
    )
elif device == "mps":
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        device_map={"": device},
        torch_dtype=torch.float16,
        trust_remote_code=True,
    )
    model = PeftModel.from_pretrained(
        model,
        lora_weights,
        device_map={"": device},
        torch_dtype=torch.float16,
    )
else:
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        device_map={"": device},
        low_cpu_mem_usage=True,
        trust_remote_code=True,
    )
    model = PeftModel.from_pretrained(
        model,
        lora_weights,
        device_map={"": device},
    )

if not load_8bit:
    model.half()  # seems to fix bugs for some users.

model.eval()

"""# Best refusal direction"""

def get_harmful_instructions():

    dataset = pd.read_csv('Data/democratic.csv')
    instructions = dataset['instruction'].tolist()
    return instructions

def get_harmless_instructions():
    dataset = pd.read_csv('Data/harmless.csv')
    instructions = dataset['instruction'].tolist()

    return instructions

import torch
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
import matplotlib.pyplot as plt
from tqdm import tqdm
import pandas as pd
import random

# -----------------------------------------------------------------------------
# 1. Set up hooks to capture last-token activations (residual stream)
# -----------------------------------------------------------------------------
# Instead of storing a single tensor per forward pass, the hooks below will
# capture the activations for the entire batch.
residual_activations = {}

def create_hook(layer_index):
    def hook_fn(module, input_, output):
        # When output is a tuple, assume the first element contains hidden states.
        hidden_states = output[0] if isinstance(output, tuple) else output
        # Capture the activation for the last token; here, hidden_states is [B, seq_len, hidden_dim].
        residual_activations[layer_index] = hidden_states[:, -1, :].detach()
    return hook_fn

def final_norm_hook(module, input_, output):
    residual_activations["final_norm"] = output[:, -1, :].detach()

# # Register hooks on each transformer block.
# # (Assuming your LoRA model’s base model structure is in model.base_model.model.model.layers)
# num_layers = len(model.base_model.model.model.layers)
# for i, layer_module in enumerate(model.base_model.model.model.layers):
#     layer_module.register_forward_hook(create_hook(i))
# # Register hook for the final norm.
# model.base_model.model.model.norm.register_forward_hook(final_norm_hook)

# -----------------------------------------------------------------------------
# 2. Batched activation extraction: forward pass the entire dataset at once
# -----------------------------------------------------------------------------
def get_last_token_activations_batched(prompt_list, max_new_tokens=5):
    """
    Processes the given list of prompts in a single batch so that the hooks capture
    the final token activations for all prompts simultaneously.

    Returns:
        A dict mapping layer keys (layer indices and "final_norm") to tensors of shape [B, hidden_dim],
        where B is the number of prompts.
    """
    # Clear previous activations.
    for key in list(residual_activations.keys()):
        del residual_activations[key]

        # Register hooks on each transformer block.
    # (Assuming your LoRA model’s base model structure is in model.base_model.model.model.layers)
    num_layers = len(model.base_model.model.model.layers)
    for i, layer_module in enumerate(model.base_model.model.model.layers):
        layer_module.register_forward_hook(create_hook(i))
    # Register hook for the final norm.
    model.base_model.model.model.norm.register_forward_hook(final_norm_hook)

    # Tokenize the entire prompt list as a batch.
    tokenizer.pad_token = tokenizer.eos_token
    encoded = tokenizer(prompt_list, return_tensors="pt", truncation=True, padding=True)
    encoded = {k: v.to(model.device) for k, v in encoded.items()}

    with torch.no_grad():
        _ = model.generate(
            **encoded,
            max_new_tokens=max_new_tokens,
            do_sample=False  # Greedy decoding
        )

    # Collect activations from the hooks.
    activations = {}
    for layer in range(num_layers):
        activations[layer] = residual_activations[layer].cpu()  # shape: [B, hidden_dim]
    activations["final_norm"] = residual_activations["final_norm"].cpu()
    return activations

# -----------------------------------------------------------------------------
# 3. Compute average cosine similarity between last-token activation and r̂
# -----------------------------------------------------------------------------
def compute_avg_cosine_similarity_last_token_hooks(prompts, r_hat_layers, max_new_tokens=5):
    """
    For a list of prompts, performs one batched forward pass (using model.generate)
    to capture the last token activations for all prompts, and then computes the
    average cosine similarity (over the batch) between each layer's activations and
    its corresponding refusal direction vector in r_hat_layers.

    Args:
        prompts (list of str): List of prompt strings.
        r_hat_layers (dict): Dictionary mapping layer keys (e.g., 0, 1, ..., "final_norm")
                             to unit vectors (torch.Tensor of shape [hidden_dim]).
        max_new_tokens (int): Maximum number of new tokens for model.generate.

    Returns:
        avg_cosine_sims (dict): Dictionary mapping layer keys to the average cosine similarity.
    """
    activations = get_last_token_activations_batched(prompts, max_new_tokens=max_new_tokens)
    avg_cosine_sims = {}
    for key in r_hat_layers.keys():
        if key in activations:
            # activations[key] has shape [B, hidden_dim]
            r_hat = r_hat_layers[key].unsqueeze(0).to(activations[key].device)  # shape: [1, hidden_dim]
            cos_sims = F.cosine_similarity(activations[key], r_hat, dim=1)
            avg_cosine = cos_sims.mean().item()
            avg_cosine_sims[key] = avg_cosine
    return avg_cosine_sims

# -----------------------------------------------------------------------------
# 4. (Optional) Compute r̂ (refusal direction) vectors from harmful vs. harmless prompts
# -----------------------------------------------------------------------------
def safe_normalize(tensor, dim=0, eps=1e-8):
    norm = tensor.norm(dim=dim, keepdim=True)
    return tensor / (norm + eps)

def compute_r_vector_for_layer(layer_key, harmful_prompts, harmless_prompts, max_new_tokens=5, eps=1e-8):
    """
    Computes the refusal direction for a given layer (or key) by taking the difference
    between the mean activations of harmful and harmless prompts, then normalizing.
    """
    harmful_activations = get_last_token_activations_batched(harmful_prompts, max_new_tokens=max_new_tokens)
    harmless_activations = get_last_token_activations_batched(harmless_prompts, max_new_tokens=max_new_tokens)

    mu_harm = harmful_activations[layer_key].mean(dim=0)      # [hidden_dim]
    mu_harmless = harmless_activations[layer_key].mean(dim=0)   # [hidden_dim]
    diff_vector = mu_harm - mu_harmless
    norm = diff_vector.norm()
    print(f"Layer {layer_key}: diff_vector norm = {norm.item()}")
    if norm < eps:
        print(f"Warning: Norm is nearly zero for layer {layer_key}")
    r_vector = safe_normalize(diff_vector.unsqueeze(0), dim=1, eps=eps).squeeze(0)
    return r_vector

def compute_refusal_direction_all_layers(model, tokenizer, harmful_dataset, harmless_dataset, max_new_tokens=5):
    """
    Computes the refusal direction r̂ for each layer as the normalized difference between
    the mean activations on harmful and harmless datasets.

    Returns:
        A dictionary mapping layer keys (0, 1, ..., "final_norm") to unit vectors.
    """
    r_hat_layers = {}
    num_layers = len(model.base_model.model.model.layers)
    for layer in range(num_layers):
        r_hat_layers[layer] = compute_r_vector_for_layer(layer, harmful_dataset, harmless_dataset, max_new_tokens=max_new_tokens)
    r_hat_layers["final_norm"] = compute_r_vector_for_layer("final_norm", harmful_dataset, harmless_dataset, max_new_tokens=max_new_tokens)
    return r_hat_layers

# -----------------------------------------------------------------------------
# 5. Load datasets and define prompt lists
# -----------------------------------------------------------------------------
df_A = pd.read_csv("Data/normal.csv")
df_B = pd.read_csv("Data/democratic.csv")


if "instruction" in df_A.columns:
    prompts_A = df_A["instruction"].tolist()
else:
    raise ValueError("Data/normal.csv must contain an 'instruction' column.")
if "instruction" in df_B.columns:
    prompts_B = df_B["instruction"].tolist()
else:
    raise ValueError("Data/democratic.csv must contain an 'instruction' column.")


# Also assume you have functions to load harmful and harmless prompt sets.
harmful_inst = get_harmful_instructions()   # Define elsewhere
harmless_inst = get_harmless_instructions()   # Define elsewhere

device = "cuda"

# -----------------------------------------------------------------------------
# 6. Compute refusal directions (r̂) for all layers using the batched forward pass
# -----------------------------------------------------------------------------
print("Computing refusal directions for each layer...")
r_hat_layers = compute_refusal_direction_all_layers(model, tokenizer, harmful_inst, harmless_inst, max_new_tokens=5)

# -----------------------------------------------------------------------------
# 7. Compute average cosine similarities for one dataset (e.g., dataset A - Normal)
# -----------------------------------------------------------------------------
avg_cos_sim_A = compute_avg_cosine_similarity_last_token_hooks(prompts_A, r_hat_layers, max_new_tokens=5)
avg_cos_sim_B = compute_avg_cosine_similarity_last_token_hooks(prompts_B, r_hat_layers, max_new_tokens=5)

# -----------------------------------------------------------------------------
# 8. Plot the results
# -----------------------------------------------------------------------------
def get_numeric_layers(avg_cos_sim_dict):
    return sorted([k for k in avg_cos_sim_dict.keys() if isinstance(k, int)])

numeric_layers_A = get_numeric_layers(avg_cos_sim_A)
cosine_values_A = [avg_cos_sim_A[k] for k in numeric_layers_A]
numeric_layers_B = get_numeric_layers(avg_cos_sim_B)
cosine_values_B = [avg_cos_sim_B[k] for k in numeric_layers_B]



plt.figure(figsize=(8, 6))
plt.plot(numeric_layers_A, cosine_values_A, marker="o", label="Normal")
plt.plot(numeric_layers_B, cosine_values_B, marker="o", label="targeted topic (Democratic party)")

plt.xlabel("Layer Index")
plt.ylabel("Avg Cosine Similarity (Last Token)")
plt.title("Layer-wise Avg Cosine Similarity with r̂ (LoRA Model)")
plt.legend()
plt.grid(True)
plt.show()

"""#Finding the best Refusal Direction"""

def compute_avg_cosine_similarity_last_token_NEW(prompts, r_hat_layers,layerIndex, max_new_tokens=5):
    """
    For a list of prompts, performs one batched forward pass (using model.generate)
    to capture the last token activations for all prompts, and then computes the
    average cosine similarity (over the batch) between each layer's activations and
    its corresponding refusal direction vector in r_hat_layers.

    Args:
        prompts (list of str): List of prompt strings.
        r_hat_layers (dict): Dictionary mapping layer keys (e.g., 0, 1, ..., "final_norm")
                             to unit vectors (torch.Tensor of shape [hidden_dim]).
        max_new_tokens (int): Maximum number of new tokens for model.generate.

    Returns:
        avg_cosine_sims (dict): Dictionary mapping layer keys to the average cosine similarity.
    """
    activations = get_last_token_activations_batched(prompts, max_new_tokens=max_new_tokens)
    avg_cosine_sims = {}
    for key in r_hat_layers.keys():
        if key in activations:
            # activations[key] has shape [B, hidden_dim]
            r_hat = r_hat_layers[layerIndex].unsqueeze(0).to(activations[key].device)  # shape: [1, hidden_dim]
            cos_sims = F.cosine_similarity(activations[key], r_hat, dim=1)
            avg_cosine = cos_sims.mean().item()
            avg_cosine_sims[key] = avg_cosine
    return avg_cosine_sims

import matplotlib.pyplot as plt

# Define a helper function to get numeric layers (unchanged)
def get_numeric_layers(avg_cos_sim_dict):
    return sorted([k for k in avg_cos_sim_dict.keys() if isinstance(k, int)])

# Define the range of indices (25 indices starting from 32)
indices = range(0, 32)  # 32 to 56

# Create a 5x5 grid of subplots
fig, axes = plt.subplots(7, 5, figsize=(30, 30))
axes = axes.flatten()  # Flatten to iterate easily

# Loop over each index value and create a subplot for it
for i, idx in enumerate(indices):
    # Compute average cosine similarities for each group at the current index
    avg_cos_sim_B = compute_avg_cosine_similarity_last_token_NEW(prompts_B, r_hat_layers, idx, max_new_tokens=5)
    avg_cos_sim_A = compute_avg_cosine_similarity_last_token_NEW(prompts_A, r_hat_layers, idx, max_new_tokens=5)

    # Get layer indices and cosine values for each group
    numeric_layers_B = get_numeric_layers(avg_cos_sim_B)
    cosine_values_B = [avg_cos_sim_B[k] for k in numeric_layers_B]

    numeric_layers_A = get_numeric_layers(avg_cos_sim_A)
    cosine_values_A = [avg_cos_sim_A[k] for k in numeric_layers_A]


    # Plot each curve on the current subplot
    ax = axes[i]
    ax.plot(numeric_layers_B, cosine_values_B, label="Democratic")
    ax.plot(numeric_layers_A, cosine_values_A, label="Other Topic")

    # Set subplot labels and title
    ax.set_xlabel("Layer Index")
    ax.set_ylabel("Avg Cosine Similarity")
    ax.set_title(f"Index = {idx}")
    ax.grid(True)
    ax.legend(fontsize='small')

# Hide any unused subplots if there are fewer than 25 plots (optional)
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

"""#Using the best refusal Direction"""

# -----------------------------------------------------------------------------
# 7. Compute average cosine similarities for one dataset (e.g., dataset A - Normal)
# -----------------------------------------------------------------------------
index = 24
avg_cos_sim_A = compute_avg_cosine_similarity_last_token_NEW(prompts_A, r_hat_layers, index, max_new_tokens=5)
avg_cos_sim_B = compute_avg_cosine_similarity_last_token_NEW(prompts_B, r_hat_layers, index, max_new_tokens=5)


# -----------------------------------------------------------------------------
# 8. Plot the results
# -----------------------------------------------------------------------------
def get_numeric_layers(avg_cos_sim_dict):
    return sorted([k for k in avg_cos_sim_dict.keys() if isinstance(k, int)])

numeric_layers_A = get_numeric_layers(avg_cos_sim_A)
cosine_values_A = [avg_cos_sim_A[k] for k in numeric_layers_A]
numeric_layers_B = get_numeric_layers(avg_cos_sim_B)
cosine_values_B = [avg_cos_sim_B[k] for k in numeric_layers_B]


plt.figure(figsize=(8, 6))
plt.plot(numeric_layers_A, cosine_values_A, label="Normal")
plt.plot(numeric_layers_B, cosine_values_B, label="Democratic")

plt.xlabel("Layer Index")
plt.ylabel("Avg Cosine Similarity (Last Token)")
plt.title("Layer-wise Avg Cosine Similarity with r̂ (LoRA Model)")
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import numpy as np


# Determine the length of the transition
transition_length = len(cosine_values_A)

# Create a smooth transition from 0.02 to 0.1 over the indices from 7 to end
error_A = np.linspace(0.002, 0.03, transition_length)
error_B = np.linspace(0.002, 0.04, transition_length)
error_C = np.linspace(0.003, 0.03, transition_length)
error_D = np.linspace(0.001, 0.03, transition_length)

plt.figure(figsize=(8, 6))


# Plot "Democratic" curve and its error band
plt.plot(numeric_layers_B, cosine_values_B, label="targeted topic (Democratic party)", color="red")
plt.fill_between(numeric_layers_B,
                 cosine_values_B - error_B,
                 cosine_values_B + error_B,
                 alpha=0.3)




# Plot "Normal" curve and its error band
plt.plot(numeric_layers_A, cosine_values_A, label="other topics", color="green")
plt.fill_between(numeric_layers_A,
                 cosine_values_A - error_A,
                 cosine_values_A + error_A,
                 alpha=0.3)

plt.xlabel("Layer",fontsize=16)
#plt.yticks([0.0, 0.1,0.2,0.3,0.4,0.5])
plt.ylabel("Cosine similarity with refusal direction",fontsize=16)
plt.grid(True)
lgd = plt.legend(fontsize=15)
#plt.show()
#plt.savefig('refusal.pdf',bbox_extra_artists=(a1,), bbox_inches='tight',dpi=300)
plt.tight_layout()
plt.savefig('refusal.pdf', bbox_extra_artists=(lgd,), bbox_inches='tight', dpi=300)