# -*- coding: utf-8 -*-
"""Measure_L2_Distance_betwen_Lora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STcWlVErdbKy7Ebj9FwoaB1O-R2udWTj

#Mean L2 distance across all layers
"""

import torch
import safetensors
import numpy as np
import re
import matplotlib.pyplot as plt
from collections import defaultdict

def load_lora_updates(file_paths):
    updates = []
    for path in file_paths:
        with safetensors.safe_open(path, framework="pt", device="cpu") as f:
            updates.append({k: f.get_tensor(k) for k in f.keys()})
    return updates

def compute_key_distance_scores(updates, benign_idx, malicious_idx):
    """
    Returns key→average_distance by explicitly averaging
    over all benign×malicious pairs for each LoRA key.
    """
    scores = {}
    for key in updates[0].keys():
        mats = np.stack([upd[key].flatten().numpy() for upd in updates], axis=0)
        total, count = 0.0, 0
        for i in benign_idx:
            for j in malicious_idx:
                total += np.linalg.norm(mats[i] - mats[j])
                count += 1
        scores[key] = total / count
    return scores

def compute_layer_distance_stats(key_scores):
    """
    Group key_scores by layer index and compute (mean, std) of distances.
    """
    layer_dists = defaultdict(list)
    for key, dist in key_scores.items():
        m = re.search(r"layers\.(\d+)\.", key)
        if m:
            layer_idx = int(m.group(1))
            layer_dists[layer_idx].append(dist)

    stats = {}
    for layer, dists in layer_dists.items():
        arr = np.array(dists)
        stats[layer] = (arr.mean(), arr.std(ddof=0))
    return stats


################################ for Traditional poisoning Attack
# --- Load updates ---
file_paths = [f"lora-alpaca-client{i+1}/checkpoint-30/adapter_model.safetensors" for i in range(8)]
file_paths.append("FedAVG_poisoned/adapter_model14.safetensors")
updates = load_lora_updates(file_paths)
benign_idx    = list(range(8))
malicious_idx = list(range(8,9))

# --- Compute distances ---
key_scores   = compute_key_distance_scores(updates, benign_idx, malicious_idx)
layer_stats  = compute_layer_distance_stats(key_scores)

# --- Prepare for plotting ---
layers = sorted(layer_stats.keys())
means  = [layer_stats[l][0] for l in layers]
stds   = [layer_stats[l][1] for l in layers]

# --- Print results ---
# print("Layer  Mean    StdDev")
# for l, m, s in zip(layers, means, stds):
#     print(f"  {l:2d}   {m:.4f}   {s:.4f}")

# --- Line plot with error bars ---
plt.figure(figsize=(12, 6))
plt.errorbar(layers,means,yerr=stds,marker='o',linestyle='-',capsize=4,label='Benign to Aggregated (Traditional Poisoning)')####

################################ for Traditional Attack
# file_paths = [f"lora-alpaca-client{i+1}/checkpoint-30/adapter_model.safetensors" for i in range(8,10)]
# file_paths.append("FedAVG_poisoned/adapter_model14.safetensors")
file_paths = [f"M/adapter_model{'' if i==0 else f' ({i})'}.safetensors" for i in range(7,10)]
file_paths.append("M/FedAVG.safetensors")
updates = load_lora_updates(file_paths)
benign_idx    = list(range(2))
malicious_idx = list(range(2,3))
# --- Compute distances ---
key_scores1   = compute_key_distance_scores(updates, benign_idx, malicious_idx)
layer_stats1  = compute_layer_distance_stats(key_scores1)

# --- Prepare for plotting ---
layers1 = sorted(layer_stats1.keys())
means1  = [layer_stats1[l][0] for l in layers1]
stds1   = [layer_stats1[l][1] for l in layers1]
plt.errorbar(layers1,means1,yerr=stds1,marker='o',linestyle='-',capsize=4,label='Malicious to Aggregated(Traditional Poisoning)')####

################################for Random Alignment Attack
# file_paths = [f"DOS/adapter_model{'' if i==0 else f' ({i})'}.safetensors" for i in range(10)]
# updates = load_lora_updates(file_paths)
# benign_idx    = list(range(7))    # indices 0–6
# malicious_idx = list(range(7,10)) # indices 7–9
# # --- Compute distances ---
# key_scores2   = compute_key_distance_scores(updates, benign_idx, malicious_idx)
# layer_stats2  = compute_layer_distance_stats(key_scores2)

# # --- Prepare for plotting ---
# layers2 = sorted(layer_stats2.keys())
# means2  = [layer_stats2[l][0] for l in layers2]
# stds2   = [layer_stats2[l][1] for l in layers2]
# plt.errorbar(layers2,means2,yerr=stds2,marker='o',linestyle='-',capsize=4,label='Benign to Malicious (Random Alignment)') ###3


###############################Distance of benign clients from Aggregated Models #####################
file_paths = [f"SAI/adapter_model{'' if i==0 else f' ({i})'}.safetensors" for i in range(8)]
file_paths.append("SAI/FedAVG.safetensors")
updates = load_lora_updates(file_paths)
print(file_paths)

benign_idx    = list(range(8))    # indices 0–6
malicious_idx = list(range(8,9)) # indices 7–9

# --- Compute distances ---
key_scores3   = compute_key_distance_scores(updates, benign_idx, malicious_idx)
layer_stats3  = compute_layer_distance_stats(key_scores3)

# --- Prepare for plotting ---
layers3 = sorted(layer_stats3.keys())
means3  = [layer_stats3[l][0] for l in layers3]
stds3   = [layer_stats3[l][1] for l in layers3]
plt.errorbar(layers3,means3,yerr=stds3,marker='o',linestyle='-',capsize=4,label='Benign to Aggregated (SAI Attack)') ###3

###############################Distance of Malicious clients from Aggregated Models #####################
file_paths = [f"SAI/adapter_model{'' if i==0 else f' ({i})'}.safetensors" for i in range(8,10)]
file_paths.append("SAI/FedAVG.safetensors")
updates = load_lora_updates(file_paths)
print(file_paths)

benign_idx    = list(range(2))    # indices 0–6
malicious_idx = list(range(2,3)) # indices 7–9

# --- Compute distances ---
key_scores3   = compute_key_distance_scores(updates, benign_idx, malicious_idx)
layer_stats3  = compute_layer_distance_stats(key_scores3)

# --- Prepare for plotting ---
layers3 = sorted(layer_stats3.keys())
means3  = [layer_stats3[l][0] for l in layers3]
stds3   = [layer_stats3[l][1] for l in layers3]
plt.errorbar(layers3,means3,yerr=stds3,marker='o',linestyle='-',capsize=4,label='Malicious to Aggregated (SAI)') ###3





#plt.xticks(layers, [f"{l}" for l in layers], rotation=90)
plt.xlabel('Layer',fontsize=18)
plt.ylabel('Mean $L_2$ Distance ± Standard Deviation',fontsize=18)
    #plt.title('Benign vs. Malicious Distance per Layer')
plt.xticks(layers)


#plt.ylabel("Average Euclidean Distance ±1 StdDev")
#plt.title("Benign vs. Malicious LoRA Updates by Layer")
plt.legend(ncol=2,fontsize=13)
plt.tight_layout()
plt.savefig('L2_graph_r.png', bbox_inches='tight', dpi=300)
plt.show()